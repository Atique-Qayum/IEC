{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of exercise_decisiontrees.ipynb","provenance":[{"file_id":"1nuvNjHL0UCNQyO6N0HO7CG1xrt34yrKF","timestamp":1618612187854}],"collapsed_sections":["tFTPXuepTyxO","akwNIGyszKeq","4BDHo_25_Ubs","jZG_bpUfOgy4","HCP75kqnrMXH","WjdVKN-jYOYL","kEIvxX3DMOkB","9uud8yEXu0v7","6nzPXJYzviYV"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vJHofdgp6NJ3"},"source":["# Decision Tree Exercises\n","\n","In this notebook we will have you solve some DT-related problems. We will provide the solution as a hidden cell under the same question cell. Ready? Begin!"]},{"cell_type":"markdown","metadata":{"id":"9aZHF91pM9hO"},"source":["# One: ROC & ROC-AUC\n","\n","In decision trees, we need to have a good measure for the success of oru classification at each branch. We have used these measures before, these measrues include:\n","\n","* Precision/Recall\n","* Accuracy\n","* F-Score (F1)\n","* Qrecall\n","* Pearson Correlation\n","* ROC\n","* ROC-AUC\n","\n","\n","Let's define ROC and ROC-AUC.\n","\n","ROC stands for *receiver operating charactristics*. They are useful for predicting the binary outcomes. In ROC, Xs are false positive rate and Ys are true postiive are true postive rates. AUC stands for AREA UNDER CURVE and is the integral of ROC. \n","\n","$ ROC = \\{(y_{FP}, y_{TP}) \\forall  y = W^TX\\}$\n","\n","$AUC_{ROC} = \\int ROC $\n","\n","Where:\n","\n","`True Positive Rate = True Positives / (True Positives + False Negatives)`\n","\n","And\n","\n","`False Positive Rate = False Positives / (False Positives + True Negatives)`\n","\n","Now, run this cell to wget the Iris dataset:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZL4vPmW-S2gY","executionInfo":{"status":"ok","timestamp":1615181089078,"user_tz":-210,"elapsed":1247,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"3a77c27e-92b7-42ed-f25e-c117b05c91c4"},"source":["!wget https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-03-08 05:24:49--  https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3858 (3.8K) [text/plain]\n","Saving to: ‘iris.csv’\n","\n","iris.csv            100%[===================>]   3.77K  --.-KB/s    in 0s      \n","\n","2021-03-08 05:24:50 (56.3 MB/s) - ‘iris.csv’ saved [3858/3858]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"EPXEiTPRS4QZ","executionInfo":{"status":"ok","timestamp":1615183741265,"user_tz":-210,"elapsed":1233,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"f34472de-abc2-4709-d50c-3677029b8e6f"},"source":["import pandas as pd\n","\n","df_full = pd.read_csv(\"/content/iris.csv\")\n","\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.1</td>\n","      <td>3.5</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.6</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5.0</td>\n","      <td>3.6</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   sepal_length  sepal_width  petal_length  petal_width species\n","0           5.1          3.5           1.4          0.2  setosa\n","1           4.9          3.0           1.4          0.2  setosa\n","2           4.7          3.2           1.3          0.2  setosa\n","3           4.6          3.1           1.5          0.2  setosa\n","4           5.0          3.6           1.4          0.2  setosa"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"b1hANXPoTEX_"},"source":["Your task is:\n","\n","1. Convert `species` to numerical labels. Selectw two labels.\n","2. Shuffle the dataset.\n","3. Import `from sklearn.tree import DecisionTreeClassifier` and use it without any hyperparams to classify Iris, sans Setosa.\n","4. Draw the confusion matrix.\n","5. Use matplotlib to draw this supposed ROC curve.\n","6. Calculate the AUC.\n","\n","Reveal the following cell for the solution of this problem."]},{"cell_type":"markdown","metadata":{"id":"tFTPXuepTyxO"},"source":["# One Answer: ROC & ROC-AUC Solution"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"id":"lT8fplDKVOtm","executionInfo":{"status":"ok","timestamp":1615212067380,"user_tz":-210,"elapsed":919,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"18b020b3-d32a-4609-e14b-04b63cb29fb6"},"source":["from sklearn.utils import shuffle\n","import matplotlib.pyplot as plt\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","df = df_full[df_full[\"species\"] != \"setosa\"].reset_index()\n","\n","df[\"labels\"] = LabelEncoder().fit_transform(df[\"species\"])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :3], df.loc[:, \"labels\"], shuffle=True)\n","\n","logreg = DecisionTreeClassifier().fit(X_train, y_train)\n","\n","pred = logreg.predict(X_test)\n","\n","conf_mat = confusion_matrix(y_test, pred)\n","\n","print(conf_mat)\n","\n","y_pred_proba = logreg.predict_proba(X_test)[::,1]\n","fpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\n","auc = roc_auc_score(y_test, y_pred_proba)\n","plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n","plt.legend(loc=4)\n","plt.show()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 8  0]\n"," [ 0 17]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"PrEnFqcNnmWX"},"source":["# Two: Splitting Criteria - Gini Index, Cross-Entropy and Chi-Squared Test\n","\n","In decision trees, three metrics can be used as criteria for splitting the branching. These metrics include:\n","\n","1. Cross-Entropy\n","2. Gini Coefficient\n","3. Chi-Squared Test\n","\n","\n","## Cross-Entropy\n","\n","Cross-Entropy is defined as the information gain of two classifiers, the amount of *suprprise* earned upon realizing receiving the information of one classifier compared to another.\n","\n","If the distribution is discrete it is defined as:\n","\n","$H(p, q) = -\\frac{1}{N}\\sum_{x \\in \\mathcal{X}} p(x) \\log{q(x)} $\n","\n","Where p is the targets and q is the predictions.\n","\n","Note: Cross Entropy is also called *log-loss*.\n","\n","## Gini Coefficient\n","\n","Gini Coefficient is an index of impurity and measures the divergence of probabability and for an array x (in our case, the distribution) it is defined as:\n","\n","$Gini(x) = \\frac{\\sum_{i=1}^{n}(2i - n - 1) x_i}{n\\sum_{i=1}^nx_i}$\n","\n","## Chi-Squared Test\n","\n","Chi-Squared test is a measure of information gain and is defined as:\n","\n","$ \\mathcal{X}^2 = \\sum_{i} \\frac{(X_i - E[X_i])^2}{E[X_i]}$\n","\n","Where X is the observed value and E[X] is the expected value. \n","\n","Use a Decision Tree Classifier from Scikit-Learn and use these metrics to compute the error."]},{"cell_type":"markdown","metadata":{"id":"akwNIGyszKeq"},"source":["#Two Answer: Cross-Entropy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LxlE12A9zULA","executionInfo":{"status":"ok","timestamp":1615229913396,"user_tz":-210,"elapsed":974,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"6b6f2ace-b071-431b-ac9b-02cf4a17a136"},"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import log_loss\n","\n","df_lab = df_full[df_full[\"species\"] != \"setosa\"].reset_index()\n","\n","df_lab[\"labels\"] = LabelEncoder().fit_transform(df[\"species\"])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :3], df.loc[: , \"labels\"])\n","\n","p = DecisionTreeClassifier().fit(Xp_train, yp_train)\n","\n","p_predict = p.predict(X_test)\n","\n","\n","log_loss(y_test, p_predict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.3815510557964281"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"4BDHo_25_Ubs"},"source":["# Two Answer: Gini Coefficient"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HsJt_8rQMGTp","executionInfo":{"status":"ok","timestamp":1615229959964,"user_tz":-210,"elapsed":1047,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"bfde662d-8b34-4d55-80d1-3bfc1a131b66"},"source":["array = df_lab.loc[:, \"labels\"]\n","\n","if np.amin(array) < 0:\n","  array -= np.amin(array) \n","\n","array += 0.0000001\n","array = np.sort(array)\n","index = np.arange(1,array.shape[0]+1)\n","n = array.shape[0]\n","gini_coeff = ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) \n","\n","gini_coeff"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.49999990000001987"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"jZG_bpUfOgy4"},"source":["# Two Answer: Chi-Squared Test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuA_rlMOQYpw","executionInfo":{"status":"ok","timestamp":1615231867076,"user_tz":-210,"elapsed":1741,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"aad1d168-4908-4e69-f9e6-5a388ec80f6c"},"source":["from scipy.stats import chisquare\n","\n","chisquare(df_lab.loc[:, \"labels\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Power_divergenceResult(statistic=49.999990000002015, pvalue=0.9999900545973714)"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"markdown","metadata":{"id":"sFcCimg0mxuL"},"source":["# Three: Data Impution using k-NN\n","\n","Sometimes when dealing with datasets, certain records are missing:\n","\n","|X|Y|Z|\n","--|--|--|\n","23|NaN|12|\n","23|13|NaN|\n","23|134|313|\n","\n","We can use k-NN to impute these missing values. You implement it using `from sklearn.impute import KNNImputer`.\n","\n","1. Load `sample_data/california_housing_test.csv`.\n","2. Randomly turn some values to NaN.\n","3. Use Sklearn's K-NN imputer."]},{"cell_type":"markdown","metadata":{"id":"HCP75kqnrMXH"},"source":["#Three Answer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OVl_UqXJrPjf","executionInfo":{"status":"ok","timestamp":1615241593431,"user_tz":-210,"elapsed":873,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"9f532634-3c26-498b-b04d-6e587e1390a0"},"source":["from sklearn.impute import KNNImputer\n","from random import randint\n","import numpy as np\n","\n","df_cal = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n","\n","df_sub = df_cal.iloc[:200, :]\n","\n","for i in range(20):\n","  df_sub[randint(0, df_sub.shape[0]), randint(0, df_sub.shape[1])] = np.nan\n","\n","df_sub_imputed = KNNImputer().fit_transform(df_sub)\n","\n","df_sub_imputed"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([[-1.2205e+02,  3.7370e+01,  2.7000e+01, ...,  6.0600e+02,\n","         6.6085e+00,  3.4470e+05],\n","       [-1.1830e+02,  3.4260e+01,  4.3000e+01, ...,  2.7700e+02,\n","         3.5990e+00,  1.7650e+05],\n","       [-1.1781e+02,  3.3780e+01,  2.7000e+01, ...,  4.9500e+02,\n","         5.7934e+00,  2.7050e+05],\n","       ...,\n","       [-1.2212e+02,  3.7450e+01,  3.8000e+01, ...,  2.8700e+02,\n","         2.0096e+00,  1.5570e+05],\n","       [-1.1948e+02,  3.6540e+01,  2.8000e+01, ...,  3.3500e+02,\n","         4.2222e+00,  1.0890e+05],\n","       [-1.2102e+02,  3.7680e+01,  2.5000e+01, ...,  5.7800e+02,\n","         3.9960e+00,  1.1450e+05]])"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"3vcK-tV4W_Nj"},"source":["# Four: Hyperparameter Tuning with Decision Trees\n","\n","Important hyperparameters for decision trees include:\n","\n","* Splitting Criterion: Which we just talked about.\n","* Max Depth: The max depth of the tree.\n","* Number of Leaves to Split Upon\n","* Max Features when splitting\n","\n","\n","Can you use Scikit-Learn's `RandomizedSearchCV` to tune these parameters?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WjdVKN-jYOYL"},"source":["#Four Answer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTW15EgyYSLr","executionInfo":{"status":"ok","timestamp":1615250562364,"user_tz":-210,"elapsed":1041,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"2e0944d9-faac-4877-aafb-ad42989236e0"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","df_lab = df_full.copy()\n","\n","df_lab[\"labels\"] = LabelEncoder().fit_transform(df_lab[\"species\"])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df_lab.iloc[:, :3], df_lab.loc[: , \"labels\"])\n","\n","\n","hyperparams = {\n","    \"criterion\": [\"gini\", \"entropy\"],\n","    \"max_depth\": range(2, 15),\n","    \"min_samples_leaf\": range(1, 10),\n","    \"min_samples_split\": range(2, 20, 2)\n","}\n","\n","clf = DecisionTreeClassifier()\n","search = RandomizedSearchCV(clf, hyperparams).fit(X_train, y_train)\n","search.best_params_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'criterion': 'gini',\n"," 'max_depth': 8,\n"," 'min_samples_leaf': 8,\n"," 'min_samples_split': 18}"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"markdown","metadata":{"id":"Y6Xk7B8A40RE"},"source":["# Five: Pipelines\n","\n","Machine Learning oeprations are not comprised of just one model. Sometimes you need to preprocess the data before training an estimator with it. That's where Scikit-Learn's `pipeline` module comes into play.\n","\n","You're not allowed to use more than one estimator. But you can add as many other models as you desrie. These models are usually scalers.\n","\n","Can you design a Sklearn pipeline with a Decision Tree and a MinMaxScaler? You can use `sample_data/california_housing_test.csv`."]},{"cell_type":"markdown","metadata":{"id":"kEIvxX3DMOkB"},"source":["#Five Answer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVCg9HVsMQ4h","executionInfo":{"status":"ok","timestamp":1615263843454,"user_tz":-210,"elapsed":1217,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"690c171f-6fdf-4277-f9eb-e03dc63240cd"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","df_lab = df_full.copy()\n","\n","df_lab[\"labels\"] = LabelEncoder().fit_transform(df_lab[\"species\"])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df_lab.iloc[:, :3], df_lab.loc[: , \"labels\"])\n","\n","\n","pipeline = Pipeline([('scaler', MinMaxScaler()), ('dt', DecisionTreeClassifier())])\n","\n","pipeline.fit(X_train, y_train)\n","\n","pipeline.predict(X_test)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2,\n","       1, 0, 2, 0, 1, 0, 2, 1, 2, 0, 1, 2, 0, 2, 0, 2])"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"markdown","metadata":{"id":"g1KZDzO-s-pM"},"source":["#Six: Feature Selection using Pearson's Correlation Coefficient\n","\n","When we have a large dataset with an exuberent number of features, we must do feature selection. When the input is numerical and the output, too, is numerical, our best bet for feature selection is Pearson's Correlation Coefficient.\n","\n","It is defined as:\n","\n","$r = \\frac{\\sum_{i = 0}^n (x_i - \\bar{x}) (y_i - \\bar{y})} {\\sqrt{\\sum_{i=0}^n(x_i - \\bar{x}) ^ 2 \\sum_{i=0}^n(y_i - \\bar{y})^2}} $\n","\n","However in Scikit-Learn you just need to use the `FeatureSelection` module. In our answer we'll use `select K best` tool along with Pearson's Correlation Coefficient.\n","\n","Can you implement it? Use `sklearn.datasets.make_regression`."]},{"cell_type":"markdown","metadata":{"id":"9uud8yEXu0v7"},"source":["# Six Answer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SQjwNlsguzr-","executionInfo":{"status":"ok","timestamp":1615305946771,"user_tz":-210,"elapsed":849,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"a2517524-aec0-42f8-b02d-67ac00b45d19"},"source":["from sklearn.datasets import make_regression\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_regression\n","X, y = make_regression(n_samples=100, n_features=100, n_informative=10)\n","fs = SelectKBest(score_func=f_regression, k=10)\n","X_selected = fs.fit_transform(X, y)\n","\n","X_selected.shape, \"Original Shape: \", X.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((100, 10), 'Original Shape: ', (100, 100))"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"gOFePZJjk_ns"},"source":["# Seven: Decision Tree Regression\n","\n","In Decision Tree Regression, instead of a class, our leaves are real values that are predicted using the usual greedy algorithm of Decision Trees.\n","\n","![Regression Trees](https://imgur.com/download/NUuxZXE/)\n","\n","This exercise is pretty straightforward.\n","\n","1. Load up `sample_data/california_housing_train`.\n","2. Load up `sklearn.tree.DecisionTreeRegressor`.\n","3. Use what we learned so far to regress `median_house_value` using any relevant feature. You will need to use feature selection, impution (if need be), pipelines and hyperpareter optimization.\n","4. Calculate MSE and MAE. If they're not desirable, change the scaler model and spend more time on tuning the hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"6nzPXJYzviYV"},"source":["#Seven Answer"]},{"cell_type":"markdown","metadata":{"id":"-MBCAr0Dvmbd"},"source":["## Feature Selection"]},{"cell_type":"code","metadata":{"id":"zzCxNlzHvsab"},"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import f_regression\n","\n","df_cali = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n","\n","X, y = df_cali.iloc[:, :-1], df_cali.iloc[:, -1]\n","\n","fs = SelectKBest(score_func=f_regression, k=5)\n","X_selected = fs.fit_transform(X, y)\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_selected, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZqYU5pg2GJV"},"source":["## Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"NaO-TfuU1YiP"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.tree import DecisionTreeRegressor\n","\n","hyperparams = {\n","    \"criterion\": [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"],\n","    \"splitter\": ['best', 'random'],\n","    \"max_depth\": range(2, 15),\n","    \"min_samples_leaf\": range(1, 10),\n","    \"min_samples_split\": range(2, 20, 2)\n","}\n","\n","\n","search = RandomizedSearchCV(DecisionTreeRegressor(), hyperparams)\n","\n","search.fit(X_train, y_train)\n","\n","best_regressor = search.best_estimator_\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XezuoLxQxMkl"},"source":["## Pipeline"]},{"cell_type":"code","metadata":{"id":"UmyUYHN_v5Q9"},"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.pipeline import Pipeline\n","\n","pipeline = Pipeline([(\"sccaler\", MinMaxScaler()), (\"estimator\", best_regressor)])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9A7kXV064MBv"},"source":["## Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWxM5ccm241d","executionInfo":{"status":"ok","timestamp":1615308414765,"user_tz":-210,"elapsed":782,"user":{"displayName":"Chubi Wubi","photoUrl":"","userId":"00422599602081718117"}},"outputId":"d0dc4f1f-8c94-4b6d-e5aa-7aa25f5f4188"},"source":["from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","pipeline.fit(X_train, y_train)\n","\n","\n","pred = pipeline.predict(X_test)\n","\n","\n","mean_absolute_error(y_test, pred), mean_squared_error(y_test, pred)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(52202.998751300336, 5498432213.670058)"]},"metadata":{"tags":[]},"execution_count":20}]}]}